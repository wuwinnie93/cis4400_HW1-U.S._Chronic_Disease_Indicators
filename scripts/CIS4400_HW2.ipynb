{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r60BYy_9sPa5",
        "outputId": "fb04dbe0-ce3b-4990-8b61-f1783e3e8237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical summary of numerical columns:\n",
            "           YearStart        YearEnd  Response     DataValue  DataValueAlt  \\\n",
            "count  210684.000000  210684.000000       0.0  2.106840e+05  2.106840e+05   \n",
            "mean     2019.983900    2020.286063       NaN  6.897924e+02  7.308139e+02   \n",
            "std         1.597365       1.073926       NaN  1.614618e+04  1.828234e+04   \n",
            "min      2015.000000    2019.000000       NaN  0.000000e+00  0.000000e+00   \n",
            "25%      2019.000000    2019.000000       NaN  1.240000e+01  1.240000e+01   \n",
            "50%      2020.000000    2020.000000       NaN  2.700000e+01  2.700000e+01   \n",
            "75%      2021.000000    2021.000000       NaN  5.830000e+01  5.830000e+01   \n",
            "max      2022.000000    2022.000000       NaN  2.925456e+06  2.925456e+06   \n",
            "\n",
            "       LowConfidenceLimit  HighConfidenceLimit  StratificationCategory2  \\\n",
            "count       190373.000000        190378.000000                      0.0   \n",
            "mean            36.866274            46.092071                      NaN   \n",
            "std             64.810910            69.765041                      NaN   \n",
            "min              0.000000             0.000000                      NaN   \n",
            "25%              9.200000            14.100000                      NaN   \n",
            "50%             19.600000            29.300000                      NaN   \n",
            "75%             40.700000            54.700000                      NaN   \n",
            "max           1427.000000          1485.900000                      NaN   \n",
            "\n",
            "       Stratification2  StratificationCategory3  Stratification3  \\\n",
            "count              0.0                      0.0              0.0   \n",
            "mean               NaN                      NaN              NaN   \n",
            "std                NaN                      NaN              NaN   \n",
            "min                NaN                      NaN              NaN   \n",
            "25%                NaN                      NaN              NaN   \n",
            "50%                NaN                      NaN              NaN   \n",
            "75%                NaN                      NaN              NaN   \n",
            "max                NaN                      NaN              NaN   \n",
            "\n",
            "          LocationID  ResponseID  StratificationCategoryID2  \\\n",
            "count  210684.000000         0.0                        0.0   \n",
            "mean       30.456973         NaN                        NaN   \n",
            "std        17.065055         NaN                        NaN   \n",
            "min         1.000000         NaN                        NaN   \n",
            "25%        17.000000         NaN                        NaN   \n",
            "50%        30.000000         NaN                        NaN   \n",
            "75%        44.000000         NaN                        NaN   \n",
            "max        78.000000         NaN                        NaN   \n",
            "\n",
            "       StratificationID2  StratificationCategoryID3  StratificationID3  \n",
            "count                0.0                        0.0                0.0  \n",
            "mean                 NaN                        NaN                NaN  \n",
            "std                  NaN                        NaN                NaN  \n",
            "min                  NaN                        NaN                NaN  \n",
            "25%                  NaN                        NaN                NaN  \n",
            "50%                  NaN                        NaN                NaN  \n",
            "75%                  NaN                        NaN                NaN  \n",
            "max                  NaN                        NaN                NaN  \n",
            "Cleaned dataset saved to: /mnt/data/Cleaned_US_Chronic_Disease_Indicators.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the dataset directly from the URL\n",
        "url = \"https://data.cdc.gov/api/views/hksd-2xuw/rows.csv?accessType=DOWNLOAD\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Define critical columns for this analysis\n",
        "critical_columns = ['YearStart', 'YearEnd', 'LocationAbbr', 'StratificationCategory1', 'Stratification1', 'DataValue']\n",
        "\n",
        "# Dropping rows with missing values in any of the critical columns\n",
        "df_cleaned = df.dropna(subset=critical_columns)\n",
        "\n",
        "# Provide a simple statistical summary for numerical columns\n",
        "print(\"Statistical summary of numerical columns:\")\n",
        "print(df_cleaned.describe())\n",
        "\n",
        "# Define the path where the cleaned dataset will be saved\n",
        "cleaned_file_path = '/mnt/data/Cleaned_US_Chronic_Disease_Indicators.csv'\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "if not os.path.exists('/mnt/data'):\n",
        "    os.makedirs('/mnt/data')\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "# Establish connection to a database (SQLite for example)\n",
        "engine = create_engine('sqlite:///chronic_disease_indicators_hw2.db')\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()\n",
        "\n",
        "# Create or update tables as per the refined model (example only)\n",
        "# Here you would define or redefine your tables similarly to what was done in Homework 1\n",
        "Base.metadata.create_all(engine)\n",
        "print(\"Database tables created/updated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSVhz16KsrX_",
        "outputId": "5be97840-0745-49e3-e62b-6589a037d8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database tables created/updated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cbd9aceb0307>:5: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base = declarative_base()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sqlalchemy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODcGBJnWsw4v",
        "outputId": "cb551a4a-231c-4559-b40a-633259d7b351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (2.0.29)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker, relationship\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "class Location(Base):\n",
        "    __tablename__ = 'locations'\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    abbreviation = Column(String(50))\n",
        "    description = Column(String(100))\n",
        "\n",
        "class Indicator(Base):\n",
        "    __tablename__ = 'indicators'\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    location_id = Column(Integer, ForeignKey('locations.id'))\n",
        "    year_start = Column(Integer)\n",
        "    data_value = Column(Float)\n",
        "    topic = Column(String(100))\n",
        "    measure = Column(String(100))\n",
        "\n",
        "    # Establish relationship with the Location table\n",
        "    location = relationship(\"Location\", back_populates=\"indicators\")\n",
        "\n",
        "# Add back_populates on the Location class for the relationship\n",
        "Location.indicators = relationship(\"Indicator\", order_by=Indicator.id, back_populates=\"location\")\n",
        "\n",
        "# Establish connection to a database (SQLite for example)\n",
        "engine = create_engine('sqlite:///chronic_disease_indicators_hw2.db')\n",
        "Base.metadata.create_all(engine)\n",
        "\n",
        "# Setup session\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL4B4Zt5vAXz",
        "outputId": "28e78071-ce8d-43c4-850e-8daaeefd2204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-8b4150afaf50>:5: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base = declarative_base()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE52-cT3vec4",
        "outputId": "97a976d5-49af-45d6-a716-72a2c7bd6a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['YearStart', 'YearEnd', 'LocationAbbr', 'LocationDesc', 'DataSource',\n",
            "       'Topic', 'Question', 'Response', 'DataValueUnit', 'DataValueType',\n",
            "       'DataValue', 'DataValueAlt', 'DataValueFootnoteSymbol',\n",
            "       'DataValueFootnote', 'LowConfidenceLimit', 'HighConfidenceLimit',\n",
            "       'StratificationCategory1', 'Stratification1', 'StratificationCategory2',\n",
            "       'Stratification2', 'StratificationCategory3', 'Stratification3',\n",
            "       'Geolocation', 'LocationID', 'TopicID', 'QuestionID', 'ResponseID',\n",
            "       'DataValueTypeID', 'StratificationCategoryID1', 'StratificationID1',\n",
            "       'StratificationCategoryID2', 'StratificationID2',\n",
            "       'StratificationCategoryID3', 'StratificationID3'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Measure' in df.columns:\n",
        "    pass\n",
        "else:\n",
        "    print(\"Measure column not found in DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ56VCoLvvIr",
        "outputId": "5d55c009-d009-4b72-816d-f20262966ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measure column not found in DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://data.cdc.gov/api/views/hksd-2xuw/rows.csv?accessType=DOWNLOAD\"\n",
        "df_raw = pd.read_csv(url)\n",
        "\n",
        "# Display the column names to verify\n",
        "print(\"Column Names:\", df_raw.columns)\n",
        "\n",
        "# Rename columns to match the data model\n",
        "df_raw.rename(columns={\n",
        "    'Topic': 'category',\n",
        "    'Question': 'indicator',\n",
        "    'DataValueType': 'data_value_type',\n",
        "    'LocationDesc': 'location'\n",
        "}, inplace=True)\n",
        "\n",
        "# Display the renamed columns to verify\n",
        "print(\"Renamed Column Names:\", df_raw.columns)\n",
        "\n",
        "# Define mapping dictionaries\n",
        "category_mapping = {\n",
        "    'Diabetes': 'Diabetes',\n",
        "    'Nutrition, Physical Activity, and Obesity': 'Nutrition and Obesity',\n",
        "    # Add other categories as needed\n",
        "}\n",
        "\n",
        "indicator_mapping = {\n",
        "    'Diabetes monitoring': 'Diabetes Monitoring',\n",
        "    'Weight status': 'Weight Status',\n",
        "    # Add other indicators as needed\n",
        "}\n",
        "\n",
        "data_value_type_mapping = {\n",
        "    'Crude prevalence': 'Crude Prevalence',\n",
        "    'Age-adjusted prevalence': 'Age-Adjusted Prevalence',\n",
        "    # Add other data value types as needed\n",
        "}\n",
        "\n",
        "location_mapping = {\n",
        "    # Assuming LocationDesc contains location descriptions\n",
        "    'NY': 'New York',\n",
        "    'CA': 'California',\n",
        "    # Add other locations as needed\n",
        "}\n",
        "\n",
        "# Create DataFrames for Each Dimension and Apply Mappings\n",
        "\n",
        "# Check if necessary columns exist\n",
        "expected_columns = ['category', 'indicator', 'data_value_type', 'location']\n",
        "missing_columns = [col for col in expected_columns if col not in df_raw.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
        "\n",
        "# Category Dimension\n",
        "unique_category_ids = df_raw['category'].unique()\n",
        "unique_category_df = pd.DataFrame(unique_category_ids, columns=['categoryId'])\n",
        "unique_category_df['categoryDescription'] = unique_category_df['categoryId'].map(category_mapping)\n",
        "unique_category_df = unique_category_df[unique_category_df['categoryDescription'].notna()]\n",
        "\n",
        "# Indicator Dimension\n",
        "unique_indicator_ids = df_raw['indicator'].unique()\n",
        "unique_indicator_df = pd.DataFrame(unique_indicator_ids, columns=['indicatorId'])\n",
        "unique_indicator_df['indicatorDescription'] = unique_indicator_df['indicatorId'].map(indicator_mapping)\n",
        "unique_indicator_df = unique_indicator_df[unique_indicator_df['indicatorDescription'].notna()]\n",
        "\n",
        "# Data Value Type Dimension\n",
        "unique_data_value_type_ids = df_raw['data_value_type'].unique()\n",
        "unique_data_value_type_df = pd.DataFrame(unique_data_value_type_ids, columns=['dataValueTypeId'])\n",
        "unique_data_value_type_df['dataValueTypeDescription'] = unique_data_value_type_df['dataValueTypeId'].map(data_value_type_mapping)\n",
        "unique_data_value_type_df = unique_data_value_type_df[unique_data_value_type_df['dataValueTypeDescription'].notna()]\n",
        "\n",
        "# Location Dimension\n",
        "unique_location_ids = df_raw['location'].unique()\n",
        "unique_location_df = pd.DataFrame(unique_location_ids, columns=['locationId'])\n",
        "unique_location_df['locationDescription'] = unique_location_df['locationId'].map(location_mapping)\n",
        "unique_location_df = unique_location_df[unique_location_df['locationDescription'].notna()]\n",
        "\n",
        "# Display the resulting DataFrames\n",
        "print(\"Category Dimension:\\n\", unique_category_df)\n",
        "print(\"Indicator Dimension:\\n\", unique_indicator_df)\n",
        "print(\"Data Value Type Dimension:\\n\", unique_data_value_type_df)\n",
        "print(\"Location Dimension:\\n\", unique_location_df)\n",
        "\n",
        "# Integrate Dimensions into the Fact Table\n",
        "fact_df = df_raw.copy()\n",
        "\n",
        "# Join with category dimension\n",
        "fact_df = fact_df.merge(unique_category_df, how='left', left_on='category', right_on='categoryId')\n",
        "fact_df = fact_df.drop(columns=['categoryId'])\n",
        "\n",
        "# Join with indicator dimension\n",
        "fact_df = fact_df.merge(unique_indicator_df, how='left', left_on='indicator', right_on='indicatorId')\n",
        "fact_df = fact_df.drop(columns=['indicatorId'])\n",
        "\n",
        "# Join with data value type dimension\n",
        "fact_df = fact_df.merge(unique_data_value_type_df, how='left', left_on='data_value_type', right_on='dataValueTypeId')\n",
        "fact_df = fact_df.drop(columns=['dataValueTypeId'])\n",
        "\n",
        "# Join with location dimension\n",
        "fact_df = fact_df.merge(unique_location_df, how='left', left_on='location', right_on='locationId')\n",
        "fact_df = fact_df.drop(columns=['locationId'])\n",
        "\n",
        "# Display the resulting fact table\n",
        "print(\"Fact Table with Dimensions:\\n\", fact_df.head())\n"
      ],
      "metadata": {
        "id": "TNCKV0EXwKPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15519e98-485c-4e0c-9d66-2bd4acd003b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names: Index(['YearStart', 'YearEnd', 'LocationAbbr', 'LocationDesc', 'DataSource',\n",
            "       'Topic', 'Question', 'Response', 'DataValueUnit', 'DataValueType',\n",
            "       'DataValue', 'DataValueAlt', 'DataValueFootnoteSymbol',\n",
            "       'DataValueFootnote', 'LowConfidenceLimit', 'HighConfidenceLimit',\n",
            "       'StratificationCategory1', 'Stratification1', 'StratificationCategory2',\n",
            "       'Stratification2', 'StratificationCategory3', 'Stratification3',\n",
            "       'Geolocation', 'LocationID', 'TopicID', 'QuestionID', 'ResponseID',\n",
            "       'DataValueTypeID', 'StratificationCategoryID1', 'StratificationID1',\n",
            "       'StratificationCategoryID2', 'StratificationID2',\n",
            "       'StratificationCategoryID3', 'StratificationID3'],\n",
            "      dtype='object')\n",
            "Renamed Column Names: Index(['YearStart', 'YearEnd', 'LocationAbbr', 'location', 'DataSource',\n",
            "       'category', 'indicator', 'Response', 'DataValueUnit', 'data_value_type',\n",
            "       'DataValue', 'DataValueAlt', 'DataValueFootnoteSymbol',\n",
            "       'DataValueFootnote', 'LowConfidenceLimit', 'HighConfidenceLimit',\n",
            "       'StratificationCategory1', 'Stratification1', 'StratificationCategory2',\n",
            "       'Stratification2', 'StratificationCategory3', 'Stratification3',\n",
            "       'Geolocation', 'LocationID', 'TopicID', 'QuestionID', 'ResponseID',\n",
            "       'DataValueTypeID', 'StratificationCategoryID1', 'StratificationID1',\n",
            "       'StratificationCategoryID2', 'StratificationID2',\n",
            "       'StratificationCategoryID3', 'StratificationID3'],\n",
            "      dtype='object')\n",
            "Category Dimension:\n",
            "   categoryId categoryDescription\n",
            "3   Diabetes            Diabetes\n",
            "Indicator Dimension:\n",
            " Empty DataFrame\n",
            "Columns: [indicatorId, indicatorDescription]\n",
            "Index: []\n",
            "Data Value Type Dimension:\n",
            " Empty DataFrame\n",
            "Columns: [dataValueTypeId, dataValueTypeDescription]\n",
            "Index: []\n",
            "Location Dimension:\n",
            " Empty DataFrame\n",
            "Columns: [locationId, locationDescription]\n",
            "Index: []\n",
            "Fact Table with Dimensions:\n",
            "    YearStart  YearEnd LocationAbbr location DataSource       category  \\\n",
            "0       2019     2019           GA  Georgia      BRFSS     Disability   \n",
            "1       2019     2019           GU     Guam      BRFSS      Arthritis   \n",
            "2       2019     2019           GU     Guam      BRFSS   Immunization   \n",
            "3       2019     2019           ME    Maine      BRFSS       Diabetes   \n",
            "4       2019     2019           NV   Nevada       NVSS  Health Status   \n",
            "\n",
            "                            indicator  Response DataValueUnit  \\\n",
            "0          Adults with any disability       NaN             %   \n",
            "1              Arthritis among adults       NaN             %   \n",
            "2  Influenza vaccination among adults       NaN             %   \n",
            "3               Diabetes among adults       NaN             %   \n",
            "4            Life expectancy at birth       NaN         Years   \n",
            "\n",
            "    data_value_type  ...  StratificationCategoryID1  StratificationID1  \\\n",
            "0  Crude Prevalence  ...                        AGE             AGE65P   \n",
            "1  Crude Prevalence  ...                        SEX               SEXF   \n",
            "2  Crude Prevalence  ...                        SEX               SEXM   \n",
            "3  Crude Prevalence  ...                        SEX               SEXM   \n",
            "4            Number  ...                    OVERALL                OVR   \n",
            "\n",
            "  StratificationCategoryID2 StratificationID2  StratificationCategoryID3  \\\n",
            "0                       NaN               NaN                        NaN   \n",
            "1                       NaN               NaN                        NaN   \n",
            "2                       NaN               NaN                        NaN   \n",
            "3                       NaN               NaN                        NaN   \n",
            "4                       NaN               NaN                        NaN   \n",
            "\n",
            "   StratificationID3 categoryDescription indicatorDescription  \\\n",
            "0                NaN                 NaN                  NaN   \n",
            "1                NaN                 NaN                  NaN   \n",
            "2                NaN                 NaN                  NaN   \n",
            "3                NaN            Diabetes                  NaN   \n",
            "4                NaN                 NaN                  NaN   \n",
            "\n",
            "   dataValueTypeDescription  locationDescription  \n",
            "0                       NaN                  NaN  \n",
            "1                       NaN                  NaN  \n",
            "2                       NaN                  NaN  \n",
            "3                       NaN                  NaN  \n",
            "4                       NaN                  NaN  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pyodbc azure-storage-blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U87mw1bjlkG4",
        "outputId": "15a3df1e-2523-4a41-902c-40111410dbde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.7/334.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.20.0-py3-none-any.whl (392 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.2/392.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Collecting azure-core>=1.28.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (42.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.11.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2024.2.2)\n",
            "Installing collected packages: pyodbc, isodate, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.30.1 azure-storage-blob-12.20.0 isodate-0.6.1 pyodbc-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import io\n",
        "\n",
        "# Azure Blob Storage connection details\n",
        "connection_string_blob = \"DefaultEndpointsProtocol=https;AccountName=cis4400spring2024hw;AccountKey=qzdZ1Xwa9ogiDzZbSaUEOomYMr4VZv131rVIZ4BUdQB6olvD4mYqbq/Ayv7PXnbgxZV7t9tGPW6b+ASt7czDxA==;EndpointSuffix=core.windows.net\"\n",
        "container_name = \"cis4400spring2024hw\"\n",
        "blob_name = \"cis4400hw1_20240407.csv\"\n",
        "\n",
        "# Initialize the BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string_blob)\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "# Verify container and blob names by listing blobs in the container\n",
        "print(\"Listing blobs in the container:\")\n",
        "blob_list = container_client.list_blobs()\n",
        "for blob in blob_list:\n",
        "    print(blob.name)\n",
        "\n",
        "# Download the blob data into a DataFrame\n",
        "download_stream = blob_client.download_blob()\n",
        "df_raw = pd.read_csv(io.StringIO(download_stream.content_as_text()))\n",
        "\n",
        "# Display the column names to verify\n",
        "print(\"Column Names:\", df_raw.columns)\n",
        "\n",
        "# Rename columns to match the data model\n",
        "df_raw.rename(columns={\n",
        "    'Topic': 'category',\n",
        "    'Question': 'indicator',\n",
        "    'DataValueType': 'data_value_type',\n",
        "    'LocationDesc': 'location'\n",
        "}, inplace=True)\n",
        "\n",
        "# Display the renamed columns to verify\n",
        "print(\"Renamed Column Names:\", df_raw.columns)\n",
        "\n",
        "# Define mapping dictionaries\n",
        "category_mapping = {\n",
        "    'Diabetes': 'Diabetes',\n",
        "    'Nutrition, Physical Activity, and Obesity': 'Nutrition and Obesity',\n",
        "    # Add other categories as needed\n",
        "}\n",
        "\n",
        "indicator_mapping = {\n",
        "    'Diabetes monitoring': 'Diabetes Monitoring',\n",
        "    'Weight status': 'Weight Status',\n",
        "    # Add other indicators as needed\n",
        "}\n",
        "\n",
        "data_value_type_mapping = {\n",
        "    'Crude prevalence': 'Crude Prevalence',\n",
        "    'Age-adjusted prevalence': 'Age-Adjusted Prevalence',\n",
        "    # Add other data value types as needed\n",
        "}\n",
        "\n",
        "location_mapping = {\n",
        "    # Assuming LocationDesc contains location descriptions\n",
        "    'NY': 'New York',\n",
        "    'CA': 'California',\n",
        "    # Add other locations as needed\n",
        "}\n",
        "\n",
        "# Check if necessary columns exist\n",
        "expected_columns = ['category', 'indicator', 'data_value_type', 'location']\n",
        "missing_columns = [col for col in expected_columns if col not in df_raw.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
        "\n",
        "# Create DataFrames for Each Dimension and Apply Mappings\n",
        "\n",
        "# Category Dimension\n",
        "unique_category_ids = df_raw['category'].unique()\n",
        "unique_category_df = pd.DataFrame(unique_category_ids, columns=['categoryId'])\n",
        "unique_category_df['categoryDescription'] = unique_category_df['categoryId'].map(category_mapping)\n",
        "unique_category_df = unique_category_df[unique_category_df['categoryDescription'].notna()]\n",
        "\n",
        "# Indicator Dimension\n",
        "unique_indicator_ids = df_raw['indicator'].unique()\n",
        "unique_indicator_df = pd.DataFrame(unique_indicator_ids, columns=['indicatorId'])\n",
        "unique_indicator_df['indicatorDescription'] = unique_indicator_df['indicatorId'].map(indicator_mapping)\n",
        "unique_indicator_df = unique_indicator_df[unique_indicator_df['indicatorDescription'].notna()]\n",
        "\n",
        "# Data Value Type Dimension\n",
        "unique_data_value_type_ids = df_raw['data_value_type'].unique()\n",
        "unique_data_value_type_df = pd.DataFrame(unique_data_value_type_ids, columns=['dataValueTypeId'])\n",
        "unique_data_value_type_df['dataValueTypeDescription'] = unique_data_value_type_df['dataValueTypeId'].map(data_value_type_mapping)\n",
        "unique_data_value_type_df = unique_data_value_type_df[unique_data_value_type_df['dataValueTypeDescription'].notna()]\n",
        "\n",
        "# Location Dimension\n",
        "unique_location_ids = df_raw['location'].unique()\n",
        "unique_location_df = pd.DataFrame(unique_location_ids, columns=['locationId'])\n",
        "unique_location_df['locationDescription'] = unique_location_df['locationId'].map(location_mapping)\n",
        "unique_location_df = unique_location_df[unique_location_df['locationDescription'].notna()]\n",
        "\n",
        "# Display the resulting DataFrames\n",
        "print(\"Category Dimension:\\n\", unique_category_df)\n",
        "print(\"Indicator Dimension:\\n\", unique_indicator_df)\n",
        "print(\"Data Value Type Dimension:\\n\", unique_data_value_type_df)\n",
        "print(\"Location Dimension:\\n\", unique_location_df)\n",
        "\n",
        "# Integrate Dimensions into the Fact Table\n",
        "fact_df = df_raw.copy()\n",
        "\n",
        "# Join with category dimension\n",
        "fact_df = fact_df.merge(unique_category_df, how='left', left_on='category', right_on='categoryId')\n",
        "fact_df = fact_df.drop(columns=['categoryId'])\n",
        "\n",
        "# Join with indicator dimension\n",
        "fact_df = fact_df.merge(unique_indicator_df, how='left', left_on='indicator', right_on='indicatorId')\n",
        "fact_df = fact_df.drop(columns=['indicatorId'])\n",
        "\n",
        "# Join with data value type dimension\n",
        "fact_df = fact_df.merge(unique_data_value_type_df, how='left', left_on='data_value_type', right_on='dataValueTypeId')\n",
        "fact_df = fact_df.drop(columns=['dataValueTypeId'])\n",
        "\n",
        "# Join with location dimension\n",
        "fact_df = fact_df.merge(unique_location_df, how='left', left_on='location', right_on='locationId')\n",
        "fact_df = fact_df.drop(columns=['locationId'])\n",
        "\n",
        "# Display the resulting fact table\n",
        "print(\"Fact Table with Dimensions:\\n\", fact_df.head())\n",
        "\n",
        "# Database connection URL\n",
        "database_url = 'postgresql://Yinghua:Winnie1!@cisbrauchdw.postgres.database.azure.com/postgres'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(database_url)\n",
        "\n",
        "# Load data into respective tables using SQLAlchemy\n",
        "def load_data(engine, table_name, df):\n",
        "    df.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
        "\n",
        "# Example DataFrames for dimensions and fact table (use your actual data)\n",
        "# geography_df, timeperiod_df, disease_df, indicator_df, chronicdisease_df\n",
        "\n",
        "# Load dimension tables\n",
        "load_data(engine, 'geography_dim', unique_location_df)\n",
        "load_data(engine, 'timeperiod_dim', unique_category_df)  # Adjust this based on your actual schema\n",
        "load_data(engine, 'disease_dim', unique_indicator_df)    # Adjust this based on your actual schema\n",
        "load_data(engine, 'indicator_dim', unique_data_value_type_df)  # Adjust this based on your actual schema\n",
        "\n",
        "# Load fact table\n",
        "load_data(engine, 'chronicdiseasedata_dim', fact_df)\n"
      ],
      "metadata": {
        "id": "IUlUbLMRwN1u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "6482892b-0b49-4e15-fe12-fbd6ce4504c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'azure'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-07b5e8af4f5d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlobServiceClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgHvn-jKk4lH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}